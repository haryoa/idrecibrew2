{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\SelfProject\\IndoRecipe2\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\62852\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from idrecibrew2.data import Seq2SeqDataFactory, Seq2SeqDataFactoryArgs\n",
    "from idrecibrew2.data.indonlg_tokenizer.tokenizer import IndoNLGTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.60ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = IndoNLGTokenizer.from_pretrained(\"indobenchmark/indobart-v2\")\n",
    "data_args = Seq2SeqDataFactoryArgs(\n",
    "    source_column=\"src\",\n",
    "    label_column=\"tgt\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "data_factory = Seq2SeqDataFactory(data_args=data_args)\n",
    "dtl = data_factory.produce_dataloader_from_csv(\n",
    "    csv_file=\"tests/data_dummy/test.csv\", batch_size=3, n_workers=1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\62852\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in fast_dev_run mode: will run a full train, val, test and prediction loop using 1 batch(es).\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_predict_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(fast_dev_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idrecibrew2.model import LitSeq2SeqTransformers, LitSeq2SeqTransformersArgs\n",
    "from idrecibrew2.eval import Seq2SeqTrainingEval\n",
    "from idrecibrew2.data.indonlg_tokenizer.tokenizer import IndoNLGTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_obj = Seq2SeqTrainingEval(tokenizer=tokenizer)\n",
    "lit_model_args = LitSeq2SeqTransformersArgs(\n",
    "    model_type=\"indobart-v2\",\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    optimizer_type=\"adam\",\n",
    "    learning_rate=1e-5,\n",
    ")\n",
    "lit_model = LitSeq2SeqTransformers(config=lit_model_args)\n",
    "lit_model.set_eval_object(eval_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | MBartForConditionalGeneration | 131 M \n",
      "--------------------------------------------------------\n",
      "131 M     Trainable params\n",
      "0         Non-trainable params\n",
      "131 M     Total params\n",
      "526.172   Total estimated model params size (MB)\n",
      "C:\\Users\\62852\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\62852\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1938: PossibleUserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\62852\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:486: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\62852\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it, loss=8.91, v_num=]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\SelfProject\\IndoRecipe2\\notebooks\\00002-full-forward.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/SelfProject/IndoRecipe2/notebooks/00002-full-forward.ipynb#ch0000008?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(lit_model, dtl, dtl)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:771\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=751'>752</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=752'>753</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=753'>754</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m--> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=770'>771</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=771'>772</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=772'>773</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:724\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=723'>724</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=724'>725</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=725'>726</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:812\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=809'>810</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=811'>812</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=813'>814</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=814'>815</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1237\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1232'>1233</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1234'>1235</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1236'>1237</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1238'>1239</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1239'>1240</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1324\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1321'>1322</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1322'>1323</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1323'>1324</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1354\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1351'>1352</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1352'>1353</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1353'>1354</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:269\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/fit_loop.py?line=264'>265</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/fit_loop.py?line=265'>266</a>\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/fit_loop.py?line=266'>267</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/fit_loop.py?line=267'>268</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/fit_loop.py?line=268'>269</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:205\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=206'>207</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:255\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_advance_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=252'>253</a>\u001b[0m \u001b[39mif\u001b[39;00m should_check_val:\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=253'>254</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mvalidating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=254'>255</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_validation()\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=255'>256</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=257'>258</a>\u001b[0m \u001b[39m# update plateau LR scheduler after metrics are logged\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:309\u001b[0m, in \u001b[0;36mTrainingEpochLoop._run_validation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=305'>306</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loop\u001b[39m.\u001b[39m_reload_evaluation_dataloaders()\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=307'>308</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=308'>309</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:211\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=207'>208</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=208'>209</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=210'>211</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_run_end()\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/base.py?line=211'>212</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:187\u001b[0m, in \u001b[0;36mEvaluationLoop.on_run_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=183'>184</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mepoch_end_reached()\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=185'>186</a>\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=186'>187</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_epoch_end(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outputs)\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=187'>188</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m []  \u001b[39m# free memory\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=189'>190</a>\u001b[0m \u001b[39m# hook\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:309\u001b[0m, in \u001b[0;36mEvaluationLoop._evaluation_epoch_end\u001b[1;34m(self, outputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=306'>307</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mtest_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, output_or_outputs)\n\u001b[0;32m    <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=307'>308</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=308'>309</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\u001b[39m\"\u001b[39;49m\u001b[39mvalidation_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_or_outputs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\idrecibrew2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1596\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1592'>1593</a>\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1594'>1595</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1595'>1596</a>\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1597'>1598</a>\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/62852/miniconda3/envs/idrecibrew2/lib/site-packages/pytorch_lightning/trainer/trainer.py?line=1598'>1599</a>\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32md:\\Documents\\SelfProject\\IndoRecipe2\\idrecibrew2\\model\\lit_model.py:66\u001b[0m, in \u001b[0;36mLitSeq2SeqTransformers.validation_epoch_end\u001b[1;34m(self, outputs)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Documents/SelfProject/IndoRecipe2/idrecibrew2/model/lit_model.py?line=63'>64</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, outputs):  \u001b[39m# type: ignore  # pylint disable=all\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/Documents/SelfProject/IndoRecipe2/idrecibrew2/model/lit_model.py?line=64'>65</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///d%3A/Documents/SelfProject/IndoRecipe2/idrecibrew2/model/lit_model.py?line=65'>66</a>\u001b[0m         bleu_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_obj\u001b[39m.\u001b[39mcompute_eval(outputs[\u001b[39m'\u001b[39;49m\u001b[39mpreds\u001b[39;49m\u001b[39m'\u001b[39;49m], outputs[\u001b[39m'\u001b[39m\u001b[39mtgts\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='file:///d%3A/Documents/SelfProject/IndoRecipe2/idrecibrew2/model/lit_model.py?line=66'>67</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mval_bleu\u001b[39m\u001b[39m\"\u001b[39m, bleu_score, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_model, dtl, dtl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95f30b99fefbe504a71f06ce13ca8c06569550641ec5e366cd8ec530130f11ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('idrecibrew2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
