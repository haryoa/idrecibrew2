{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data4/haryoaw_workspace/projects/2021_2/mbekk/idrecibrew2\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idrecibrew2.data import Seq2SeqDataFactory, Seq2SeqDataFactoryArgs\n",
    "from idrecibrew2.data.indonlg_tokenizer.tokenizer import IndoNLGTokenizer\n",
    "from idrecibrew2.model import LitSeq2SeqTransformers, LitSeq2SeqTransformersArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idrecibrew2.eval.training_eval import Seq2SeqTrainingEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = IndoNLGTokenizer.from_pretrained(\"indobenchmark/indogpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = LitSeq2SeqTransformersArgs(vocab_size=tokenizer.vocab_size, model_type=\"indogpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitSeq2SeqTransformers(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"Resep masak nasi\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 39935, 86, 15570, 6066, 3335]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_input_ids = torch.LongTensor([input_ids[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result = lit_model.model.generate(input_ids=tensor_input_ids, max_length=200, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 39935,    86, 15570,  6066,  3335, 39935,    86,  9597, 39935,\n",
       "            87, 34745,     2,  1559, 39935,    72,  2298, 39935,    80,  1413,\n",
       "         39935,    86, 15570,  6066,  3335, 39935,    86,  9597, 39935,    87,\n",
       "         34745, 39935,    86,  9597, 39935,    87, 34745, 39935,    86,  9597,\n",
       "         39935,    87, 34745, 39935,    81,   483,   322, 39935,    72,  2298,\n",
       "         39935,    80,  1413, 39935,    86,  9597, 39935,    87, 34745, 39935,\n",
       "            81,   483, 39954, 39935,    72,  2298,  5056, 39935,    80,  1413,\n",
       "         39935,    86,  9597, 39935,    87, 34745, 39935,    81,   483,  1813,\n",
       "           275, 39935,    82, 39953,  4506, 39935,    75,  1105, 39935,    71,\n",
       "          9051, 39935,    79,   535, 39935,    71,  5774, 39956, 39935,    79,\n",
       "           386,   396,  3311,  1064,   283, 11795, 39954,     2, 39935,    71,\n",
       "           345, 39935,    70,  3124, 39935,    75, 15871, 39935,    70, 12551,\n",
       "             2, 39935,    86,   443, 39935,    87,  3409, 39935,    70, 21576,\n",
       "         39935,    80,   276, 25641, 39935,    87,   368, 30728,   465, 39935,\n",
       "            84,   915,   322, 39935,    86,  9597, 39935,    75,  1105, 39954,\n",
       "            81, 17396,     2,     2, 39935,    86, 15570,  6066,  3335, 39935,\n",
       "            86,  9597, 39935,    87, 34745, 39935,    81,   483, 39935,    77,\n",
       "          1361, 39935,    88,  3171, 39935,    88,   274,  7215,  1464,   275,\n",
       "          4630, 39968, 39935,    39,    87,  1635,   288, 39935,    79,  1589,\n",
       "         39935,    86,  2949, 39935,    39,    81,   294,  7148, 39935,    72]])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resep masak nasi Resik Seneng _ Download Lagu Resep masak nasi Resik Seneng Resik Seneng Resik Seneng Mati - Download Lagu Resik Seneng Mati. Download full Lagu Resik Seneng Mati gratis. Nyanyi Gitar Cocok Kahan Campur, Kabehane wis di acak. Cara Bisa Gampang Bebas Rasa Sayang Bakal Lalekno Sampeyan ing Pro - Resik Gitar.Musik Resep masak nasi Resik Seneng Mati Iki Tema Takjubake.mp3 #Sugeng Kanggo Riko #Melodi D'"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   298,  1453, 10478]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqDataFactoryArgs(tokenizer=tokenizer, training_type=\"lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fct = Seq2SeqDataFactory(data_args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e55d70e9c41405a8633dd470fec2c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = data_fct.produce_dataloader_from_csv(\"data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = next(iter(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitSeq2SeqTransformers.load_from_checkpoint(\"outputs/indogpt/model-epoch=06-val_loss=1.861-val_bleu=0.000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"nasi uduk pedas >>> \")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_input_ids = torch.LongTensor([input_ids[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result = lit_model.model.generate(tensor_input_ids, max_length=200, num_beams=1, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer.decode(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasi uduk pedas\n",
      "1 piring nasi putih\n",
      "1 / 2 kg daging kambing\n",
      "1 / 2 kg kacang tanah\n",
      "1 / 2 kg kacang tanah\n",
      "1 / 2 kg cabe rawit\n",
      "1 / 2 kg cabe merah\n",
      "1 / 2 kg bawang putih\n",
      "1 / 2 kg bawang merah\n",
      "1 / 2 kg bawang putih\n",
      "1 / 2 kg kemiri\n",
      "1 / 2 kg kunyit\n",
      "1 / 2 kg jahe\n",
      "1 / 2 kg lengkuas\n",
      "1 / 2 kg kencur\n",
      "1 / 2 kg kencur\n",
      "1 / 2 kg kencur\n",
      "1 / 2 kg kencur\n",
      "1 / 2 kg cabe rawit\n",
      "1 / 2 kg bawang 1 / 2 kg bawang merah\n",
      "1 / 2 kg bawang putih\n",
      "1 / 2 kg bawang merah ||\n"
     ]
    }
   ],
   "source": [
    "print(result.replace(\" || \", \"\\n\").replace(\" >>> \", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sylveon",
   "language": "python",
   "name": "sylveon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
